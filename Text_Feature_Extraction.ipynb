{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "    \n",
    "1. <a href=\"#item31\">NLP</a> \n",
    "2. <a href=\"#item31\">Download and Import NLTK</a>  \n",
    "3. <a href=\"#item32\">Perform Data Processing</a> \n",
    "4. <a href=\"#item33\">Perform Feature Extraction</a>\n",
    "5. <a href=\"#item33\">Import pandas</a>\n",
    "6. <a href=\"#item34\">Read Dataset</a>  \n",
    "7. <a href=\"#item34\">Train and Test the model</a> \n",
    "8. <a href=\"#item34\">Model Evaluation</a>\n",
    "9. <a href=\"#item34\">Questions</a>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing (NLP)** is the ability of a computer program to understand human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "Computers are very good at handling direct numerical information and dealing with text data is problematic, since our computers, scripts can’t read and understand text in any human sense.\n",
    "\n",
    "But the main problem in working with language processing is that machine learning algorithms can not work on the raw text directly so it needs some specialized **pre-processing techniques** to understand raw text data so we can do some **feature extraction techniques** to convert text into a matrix(or vector) of features. \n",
    "\n",
    "In this section we will be discussing some of the techniques to create some structure out of highly unstructure text data using **NLTK** library.\n",
    "\n",
    "NLTK stands for **Natural Language Toolkit**, which is a commonly used NLP library with a lot of corpus, models, and algorithms. \n",
    "\n",
    "**corpus** is the collection of text documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The flow of NLP application looks like\n",
    "\n",
    "\n",
    " Text ----> Pre processing -----> Feature extraction ----> ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK\n",
    "\n",
    "The below command will open the NLTK downloader. You may download everything from the collections tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data: tokenization, stemming, and removal of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at three common pre-processing step's in natural language processing:\n",
    "\n",
    "1) **Tokenization:** the process of segmenting text into words, clauses or sentences (here we will separate out words).\n",
    "\n",
    "2) **Removal of stop words:** removal of commonly used words unlikely to be useful for learning e.g. 'a', 'and', 'but', 'what' etc.\n",
    "\n",
    "3) **Stemming:** reducing related words to a common stem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Jhon was delighted he got a part in school play, even though the part was small one. \"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nList of words:\")\n",
    "words = tokenizer.tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "words = [w for w in words if not w in stops]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "meaningful_words = []\n",
    "for w in words:\n",
    "    meaningful_words.append(stemming.stem(w))\n",
    "    print(w,\"---->\",stemming.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Feature extraction step means to extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "\n",
    "In this section we will transform tokens into features.\n",
    "\n",
    "**Some of the most popular methods of feature extraction are :**\n",
    "\n",
    "1- Bag-of-Words\n",
    "\n",
    "2- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in NLP. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity(number of times words appears in courpus).\n",
    "\n",
    "Bag-of-Words is one of the most fundamental methods to transform tokens into a set of features. The Bag of word model is used in document classification, where each word is used as a feature for training the classifier.\n",
    "\n",
    "There are 3 steps while creating a BoW model :\n",
    "\n",
    "1- The first step is **text-preprocessing**\n",
    "\n",
    "2- The second step is to **create a vocabulary** of all unique words from the corpus.\n",
    "\n",
    "3- In the third step, we **create a matrix of features** by assigning a separate column for each word, while each row corresponds to messages. This process is known as **Text Vectorization**. Each entry in the matrix signifies the presence or absence of the word in the message. We put 1 if the word is present in the review, and 0 if it is not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are ready-to-use python package for this model **\"CountVectorizer\"**, which will do all of the above 3 steps for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"Hey, let's go to the game today\",\n",
    "          \"call your sister also call your parents\",\n",
    "          \"want to go out?\"]\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below output you can see numer of times the word appears in the courpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the unique words present in the courpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = vect.get_feature_names()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "**Term frequency-Inverse document frequency** uses all the tokens in the dataset as vocabulary. Frequency of occurrence of a token from vocabulary in each document consists of the term frequency and number of documents in which token occurs determines the Inverse document frequency.What this ensures is that,if a token occurs frequently in a document that token will have high TF but if that token occurs frequently in majority of documents then it reduces the IDF, which occur frequently are penalized and important words which contain the essence of document get a boost. Both these TF and IDF matrices for a particular document are multiplied and normalized to form TF-IDF of a document.\n",
    "\n",
    "And there are ready-to-use python package for this model as well **\"TfidfTransformer\"**.\n",
    "\n",
    "But we will use **TfidfVectorizer** in our next section, which perform both **CountVectorizer** and **TfidfTransformer**\n",
    "\n",
    "\n",
    "**TfidfVectorizer = CountVectorizer + TfidfTransformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative or neutral.\n",
    "\n",
    "The most common use of Sentiment Analysis is to classifying a text to a class. Depending on the dataset and the reason, Sentiment Classification can be binary (positive or negative) or multi-class (3 or more classes) problem.\n",
    "\n",
    "**In this section, I’ll show you how to effectively perform sentiment classification on hotel reviews dataset using** **Machine Learning approach**. \n",
    "\n",
    "This approach, employees a machine-learning technique and diverse features to construct a classifier that can identify text that expresses sentiment.\n",
    "\n",
    "We have a datset of hotel reviews and it contains 210 reviews of guests and information of 2 classes, positive class(positive reviews) and negative class(negative reviews).\n",
    "\n",
    "1 stand for positive review in Label column.\n",
    "\n",
    "0 stand for negative review in the Label column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages\n",
    "\n",
    "Let's start by importing pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Pandas method <b>read_csv()</b> to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://raw.githubusercontent.com/skilluplabs/TextFeatureExtraction/master/hotel_review.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method <b>head()</b> to display the first five rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we put our Reviews into X variable and Lable into y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Reviews']\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split them to form train and test datasets. Using the train_test_split to create train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Linear SVC classifier from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clf = Pipeline([(\"tfidf\", TfidfVectorizer(stop_words ='english')),(\"svc_classifier\", LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Linear SVC classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Performance of the Model \n",
    "\n",
    "To see how the model performs on the new data (test data), we will use accuracy as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on test data:  0.873015873015873\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score on test data: ', accuracy_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, you need to apply the above pre-processing techniques on the same dataset 'hotel_review.csv' using dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url ='https://raw.githubusercontent.com/skilluplabs/TextFeatureExtraction/master/hotel_review.csv'\n",
    "reviews = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reviews)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s look at what columns exist in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reviews', 'Label']\n"
     ]
    }
   ],
   "source": [
    "print (list(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will convert all text to lower case in order to reduce the size of vocabulary by reducing the same words because NLTK is\n",
    "\n",
    "#### case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Reviews'] = reviews['Reviews'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check out the first review and it's sentiment. \n",
    "#### The review is text and the sentiment label is either 0 (negative) or 1 (positive) based on how the reviewer rated it on hotel review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_review = reviews.iloc[0]\n",
    "print(example_review['Reviews'])\n",
    "\n",
    "#Now we do same for label:\n",
    "print(example_review['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. Create a function 'identify_tokens()' to perform Tokenization using TreebankWordTokenizer on the complete dataframe(Reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def identify_tokens(row):\n",
    "    Reviews = row['Reviews']\n",
    "    tokens = tokenizer.tokenize(Reviews)\n",
    "    token_words = [w for w in tokens]\n",
    "    return token_words\n",
    "\n",
    "reviews['Reviews'] = reviews.apply(identify_tokens, axis=1)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Create a function 'remove_stops()' to remove stopwords from the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Reviews']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "reviews['Reviews'] = reviews.apply(remove_stops, axis=1)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Create a function 'stem_list()' to perform stemming on the complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "def stem_list(row):\n",
    "    my_list = row['Reviews']\n",
    "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
    "    return (stemmed_list)\n",
    "\n",
    "reviews['Reviews'] = reviews.apply(stem_list, axis=1)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thank you for completing this notebook</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
